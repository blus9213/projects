{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,LSTM,BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            LTC-USD_close     future  target\n",
      "time                                        \n",
      "1528968660      96.580002  96.500000       0\n",
      "1528968720      96.660004  96.389999       0\n",
      "1528968780      96.570000  96.519997       0\n",
      "1528968840      96.500000  96.440002       0\n",
      "1528968900      96.389999  96.470001       1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 3\n",
    "RATIO_TO_PREDECT = \"LTC-USD\"\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "NAME =f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"\n",
    "\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "\n",
    "ratios = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"]\n",
    "\n",
    "for ratio in ratios:\n",
    "    dataset = f\"C:/Users/HP/OneDrive/Desktop/python files/crypto_data/{ratio}.csv\"\n",
    "    df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "\n",
    "    if main_df.empty:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "# Calculate 'future' column after all data is processed\n",
    "if f\"{RATIO_TO_PREDECT}_close\" in main_df.columns:\n",
    "    main_df['future'] = main_df[f\"{RATIO_TO_PREDECT}_close\"].shift(-FUTURE_PERIOD_PREDICT)\n",
    "else:\n",
    "    raise KeyError(f\"Column {RATIO_TO_PREDECT}_close is missing in the DataFrame.\")\n",
    "\n",
    "\n",
    "\n",
    "main_df['target'] = list(map(classify, main_df[f\"{RATIO_TO_PREDECT}_close\"],main_df[\"future\"]))\n",
    "\n",
    "print(main_df[[f\"{RATIO_TO_PREDECT}_close\", \"future\",\"target\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproccess_df(df):\n",
    "    df = df.drop('future',axis=1)\n",
    "\n",
    "    #preprocessing data\n",
    "    for col in df.columns:\n",
    "        if col !=\"target\": #not preprocessing target column\n",
    "            df[col] = df[col].pct_change() #normalizing data \n",
    "            df.dropna(inplace=True)\n",
    "            df[col] = preprocessing.scale(df[col].values) # scaling data \n",
    "    df.dropna(inplace=True) \n",
    "\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen = SEQ_LEN )\n",
    "    \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]])\n",
    "        if len(prev_days) == SEQ_LEN:\n",
    "            sequential_data.append([np.array(prev_days),i[-1]])\n",
    "    random.shuffle(sequential_data)# shuffling data\n",
    "\n",
    "    print(\"Sequential data length:\", len(sequential_data))\n",
    "\n",
    "    if len(sequential_data) == 0:\n",
    "        raise ValueError(\"No data available after sequential processing!\")\n",
    "    \n",
    "     # balancing the data \n",
    "\n",
    "    buys = []\n",
    "    sells = []\n",
    "    for seq,target in sequential_data:\n",
    "        if target ==0:\n",
    "            sells.append([seq,target])\n",
    "        elif target ==1:\n",
    "            buys.append([seq,target])\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "\n",
    "    lower = min(len(buys),len(sells))\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "\n",
    "    sequential_data = buys + sells\n",
    "    random.shuffle(sequential_data)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        Y.append(target)\n",
    "\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(main_df.index.values)\n",
    "\n",
    "last_5pct=times[-int(0.05*len(times))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential data length: 78313\n",
      "Sequential data length: 3798\n",
      "train_x shape: (65962, 60, 8)\n",
      "train_y shape: (65962,)\n",
      "validation_x shape: (3174, 60, 8)\n",
      "validation_y shape: (3174,)\n"
     ]
    }
   ],
   "source": [
    "validation_main_df = main_df[(main_df.index >= last_5pct)]\n",
    "main_df =main_df[(main_df.index < last_5pct)]\n",
    "\n",
    "\n",
    "train_x,train_y = preproccess_df(main_df)\n",
    "validation_x,validation_y = preproccess_df(validation_main_df)\n",
    "\n",
    "# After preprocessing\n",
    "print(\"train_x shape:\", train_x.shape)\n",
    "print(\"train_y shape:\", train_y.shape)\n",
    "print(\"validation_x shape:\", validation_x.shape)\n",
    "print(\"validation_y shape:\", validation_y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 62210 -- validation :3692\n",
      "DONT buys :31105, buys31105\n",
      "VALIDATION DONT buys :1846, buys: 1846\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(train_x)} -- validation :{len(validation_x)}\")\n",
    "\n",
    "print(f\"DONT buys :{train_y.count(0)}, buys{train_y.count(1)}\")\n",
    "print(f\"VALIDATION DONT buys :{validation_y.count(0)}, buys: {validation_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive\\Desktop\\python files\\Tensorflow\\comp_vis\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\HP\\OneDrive\\Desktop\\python files\\Tensorflow\\comp_vis\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m 605/1031\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 257ms/step - accuracy: 0.5074 - loss: 0.7695"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128,input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128,input_shape=(train_x.shape[1:])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3, decay = 1e-5)\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "filepath = \"RNN_Final-{epoch:0d}\"\n",
    "checkpoint = ModelCheckpoint(\"models/{}.keras\".format(filepath,monitor = 'val_acc',verbose =1,save_best_only=True,model='max'))\n",
    "\n",
    "history = model.fit(train_x,train_y,batch_size = BATCH_SIZE, epochs=EPOCHS, validation_data=(validation_x,validation_y),callbacks = [tensorboard,checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_vis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
